# max by(job,namespace,network,address,name) is the primary key which all the metrics are aggregated by to avoid alerts flapping: i.e. when a K8s pod restarts 
# The same result could be achieved by applying max without(instance,pod)
# --
# All the queries are also filtered by the "environment" label: i.e. to not mix metrics coming from staging and production environments

{{ if and .Values.prometheusRules.enabled ( ne .Values.config.environment "ci" ) }}
{{ if and .Values.config.subscriber.modules.transferEventScanner.enabled ( ne .Values.config.environment "ci" ) }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
{{ toYaml .Values.prometheusRules.labels | indent 4 }}
  name: {{ .Release.Name }}
spec:
  groups:
  - name: {{ .Release.Name }}.rules
    rules:
    - alert: ScanStallShort
      annotations:
        message: 'Scanner got stuck for more than 20 minutes | Please check the {{`{{ $labels.namespace }}`}}/{{`{{ $labels.service }}`}} service'
      expr: max without(instance,pod) (increase(polkadot_watcher_tx_scan_height{environment="{{ .Values.config.environment }}"}[20m])) == 0
      for: 20m
      labels:
        severity: warning
        origin: {{ .Values.prometheusRules.origin }}
    - alert: ScanStallLong
      annotations:
        message: 'Scanner got stuck for more than 40 minutes | Please check the {{`{{ $labels.namespace }}`}}/{{`{{ $labels.service }}`}} service'
      expr: max without(instance,pod) (increase(polkadot_watcher_tx_scan_height{environment="{{ .Values.config.environment }}"}[20m])) == 0
      for: 40m
      labels:
        severity: critical
        origin: {{ .Values.prometheusRules.origin }}
{{ end }}     
{{ end }}
